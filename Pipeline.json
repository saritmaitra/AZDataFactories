# pipeline in JSON format

{
  "name": "PipelineName",
  "properties":
  {
    "description": "pipeline description",
    "activities":
    [
    ],
    "parameters": {
    }
  }
}

# PipelineName: Specifies the name of the pipeline. Use a name that represents the action that the pipeline performs.
# Maximum number of characters: 140.
# Must start with a letter, number, or underscore (_).
# The following characters are not allowed: . + ? / < > * % & : \
# Type: String, Required: Yes

# Description: description Specifies the text describing what theb pipeline is used for.
# Type: String, Required:  No

# Acitivities: activities The pipeline can have one or more activities defined within it.
# Activities represent a processing step in a pipeline. These are specific tasks that compose the overall pipeline. For example, we might use a
# Spark activity, which runs a Spark query against Azure Databricks or an HDInsight cluster, to transform or analyze your data. ADF
# supports three types of activities: data movement (copy activities), data transform (compute activities), and control activities.
# Type: Array, Required: Yes

# parameters The parameters property can have one or more parameters defined within the pipeline, making your pipeline flexible for reuse.
# Type: List, Requitred: No


# create a pipeline with a copy activity that uses the input and output datasets. 
# The copy activity copies data from the file you specified in the input dataset settings to the file you specified in the output dataset settings.
{
    "name": "Adfv2QuickStartPipeline",
    "properties": {
        "activities": [
            {
                "name": "CopyFromBlobToBlob",
                "type": "Copy",
                "dependsOn": [],
                "policy": {
                    "timeout": "7.00:00:00",
                    "retry": 0,
                    "retryIntervalInSeconds": 30,
                    "secureOutput": false,
                    "secureInput": false
                },
                "userProperties": [],
                "typeProperties": {
                    "source": {
                        "type": "BinarySource",
                        "storeSettings": {
                            "type": "AzureBlobStorageReadSettings",
                            "recursive": true
                        }
                    },
                    "sink": {
                        "type": "BinarySink",
                        "storeSettings": {
                            "type": "AzureBlobStorageWriteSettings"
                        }
                    },
                    "enableStaging": false
                },
                "inputs": [
                    {
                        "referenceName": "InputDataset",
                        "type": "DatasetReference"
                    }
                ],
                "outputs": [
                    {
                        "referenceName": "OutputDataset",
                        "type": "DatasetReference"
                    }
                ]
            }
        ],
        "annotations": []
    }
}


