# pipeline in JSON format

{
  "name": "PipelineName",
  "properties":
  {
    "description": "pipeline description",
    "activities":
    [
    ],
    "parameters": {
    }
  }
}

* PipelineName: Specifies the name of the pipeline. Use a name that represents the action that the pipeline performs.
# Maximum number of characters: 140.
# Must start with a letter, number, or underscore (_).
# The following characters are not allowed: . + ? / < > * % & : \
# Type: String, Required: Yes

* Description: description Specifies the text describing what theb pipeline is used for.
# Type: String, Required:  No

* Acitivities: activities The pipeline can have one or more activities defined within it.
# Activities represent a processing step in a pipeline. These are specific tasks that compose the overall pipeline. For example, we might use a
# Spark activity, which runs a Spark query against Azure Databricks or an HDInsight cluster, to transform or analyze your data. ADF
# supports three types of activities: data movement (copy activities), data transform (compute activities), and control activities.
# Type: Array, Required: Yes

* parameters The parameters property can have one or more parameters defined within the pipeline, making your pipeline flexible for reuse.
# Type: List, Requitred: No


# create a pipeline with a copy activity that uses the input and output datasets. 
# The copy activity copies data from the file you specified in the input dataset settings to the file you specified in the output dataset settings.
{
    "name": "Adfv2QuickStartPipeline",
    "properties": {
        "activities": [
            {
                "name": "CopyFromBlobToBlob",
                "type": "Copy",
                "dependsOn": [],
                "policy": {
                    "timeout": "7.00:00:00",
                    "retry": 0,
                    "retryIntervalInSeconds": 30,
                    "secureOutput": false,
                    "secureInput": false
                },
                "userProperties": [],
                "typeProperties": {
                    "source": {
                        "type": "BinarySource",
                        "storeSettings": {
                            "type": "AzureBlobStorageReadSettings",
                            "recursive": true
                        }
                    },
                    "sink": {
                        "type": "BinarySink",
                        "storeSettings": {
                            "type": "AzureBlobStorageWriteSettings"
                        }
                    },
                    "enableStaging": false
                },
                "inputs": [
                    {
                        "referenceName": "InputDataset",
                        "type": "DatasetReference"
                    }
                ],
                "outputs": [
                    {
                        "referenceName": "OutputDataset",
                        "type": "DatasetReference"
                    }
                ]
            }
        ],
        "annotations": []
    }
}


# To create the pipeline: Adfv2QuickStartPipeline, Run the Set-AzDataFactoryV2Pipeline cmdlet.
$DFPipeLine = Set-AzDataFactoryV2Pipeline `
    -DataFactoryName $DataFactory.DataFactoryName `
    -ResourceGroupName $ResGrp.ResourceGroupName `
    -Name "Adfv2QuickStartPipeline" `
    -DefinitionFile ".\Adfv2QuickStartPipeline.json"


# Create a pipeline run
# In this step, you create a pipeline run.
# Run the Invoke-AzDataFactoryV2Pipeline cmdlet to create a pipeline run. The cmdlet returns the pipeline run ID for future monitoring.
$RunId = Invoke-AzDataFactoryV2Pipeline `
  -DataFactoryName $DataFactory.DataFactoryName `
  -ResourceGroupName $ResGrp.ResourceGroupName `
  -PipelineName $DFPipeLine.Name

# Monitor the pipeline run
# Run the following PowerShell script to continuously check the pipeline run status until it finishes copying the data. 
# Copy/paste the following script in the PowerShell window, and press ENTER.
while ($True) {
    $Run = Get-AzDataFactoryV2PipelineRun `
        -ResourceGroupName $ResGrp.ResourceGroupName `
        -DataFactoryName $DataFactory.DataFactoryName `
        -PipelineRunId $RunId

    if ($Run) {
        if ($run.Status -ne 'InProgress') {
            Write-Output ("Pipeline run finished. The status is: " +  $Run.Status)
            $Run
            break
        }
        Write-Output "Pipeline is running...status: InProgress"
    }

    Start-Sleep -Seconds 10
}

# Run the following script to retrieve copy activity run details, for example, size of the data read/written.
Write-Output "Activity run details:"
$Result = Get-AzDataFactoryV2ActivityRun -DataFactoryName $DataFactory.DataFactoryName -ResourceGroupName $ResGrp.ResourceGroupName -PipelineRunId $RunId -RunStartedAfter (Get-Date).AddMinutes(-30) -RunStartedBefore (Get-Date).AddMinutes(30)
$Result

Write-Output "Activity 'Output' section:"
$Result.Output -join "`r`n"

Write-Output "Activity 'Error' section:"
$Result.Error -join "`r`n"
